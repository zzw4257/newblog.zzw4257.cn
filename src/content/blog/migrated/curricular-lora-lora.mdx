---
title: 深入理解大模型微调：从 LoRA 到 LoRA+ 的理论演进
description: 本文深入探讨了大语言模型高效微调技术 LoRA 及其改进版 LoRA+，详细解析了低秩假设、参数初始化策略以及非对称学习率对模型性能与收敛速度的影响。
pubDate: '2025-11-20T12:00:00.000Z'
image: /images/uploads/curricular-lora-lora-cover.jpg
badge: DeepDive
draft: false
categories:
  - AIGC
  - Research
tags:
  - LoRA
  - LoRA+
  - 模型微调
  - LLM
  - 深度学习
  - 参数高效微调
---

> **[迁移说明]** 本文最初发布于 `blog.zzw4257.cn`，现已迁移并在本站进行结构化整理与增强。


## 大模型微调（Fine-Tuning）概述

在大模型时代，我们通常不会从零开始训练一个模型，而是利用一个在海量数据上预训练好的、能力强大的基础模型（Foundation Model），然后在一个新的、特定的任务上对其进行“再训练”，这个过程就是**微调**。

### 1. 意义与直觉

想象一位在顶级厨艺学校毕业、精通各种烹饪技巧的厨师（预训练模型）。现在，一家高级餐厅想让他专门烹饪地方特色菜（下游特定任务）。他不需要从如何握刀、如何控制火候等基础开始学起，只需利用他已有的高超厨艺，学习并适应新菜系的食谱和风味即可。这个“学习适应”的过程，就是微调。它使得强大的通用模型能够高效地服务于各种具体应用。

### 2. 数学描述

从数学上讲，一个神经网络模型本质上是由一系列权重矩阵 $W$ 和偏置项组成的。预训练好的模型，其权重我们记为 $W_0$。

全量微调（Full Fine-Tuning）的目标是，在新的任务数据上，更新模型的所有或大部分参数，以最小化新任务的损失函数。更新后的权重 $W$ 可以表示为：

$$
W = W_0 + \Delta W
$$

这里的 $W_0$ 是预训练权重，是“冰冻”的、不参与梯度计算的。而 $\Delta W$ 是在微调过程中学习到的**权重更新量**。在全量微调中，$\Delta W$ 的维度与 $W_0$ 完全相同，这意味着对于每一个预训练参数，我们都要计算并存储它的梯度和更新。

**全量微调的困境**：对于拥有数百亿甚至数千亿参数的现代大模型（如 GPT-3），为每一个下游任务都存储一个完整的 $\Delta W$ 副本，将带来巨大的存储开销和计算负担。例如，为 10 个不同的任务微调一个 175B 参数的模型，就需要存储 10 份 175B 参数的副本，这在实际应用中是难以承受的。

---

## 第一部分：LoRA (Low-Rank Adaptation)

LoRA 的出现，正是为了解决全量微调的困境。它的核心洞见来自于一个关键假设。

### 1. 核心思想：低秩假设

LoRA 的作者们假设：**在模型适应新任务的过程中，权重的改变量 $\Delta W$ 是低秩（Low-Rank）的。**

这意味着，尽管 $\Delta W$ 与 $W_0$ 维度相同，看起来非常庞大，但它内部的信息是冗余的，其内在的“秩”（intrinsic rank）非常低。因此，我们没有必要去学习一个完整的、稠密的 $\Delta W$ 矩阵。

一个 $d \times k$ 的矩阵，如果它的秩 $r$ 远小于 $d$ 和 $k$，我们就可以通过两个更小的矩阵来近似它。具体来说，可以将 $\Delta W$ 分解（或近似）为两个细长的矩阵的乘积：

$$
\Delta W = B \cdot A
$$

其中，$B$ 是一个 $d \times r$ 的矩阵，$A$ 是一个 $r \times k$ 的矩阵，而秩 $r \ll \min(d, k)$。

通过这种方式，需要学习的参数数量从 $d \times k$ 急剧下降到 $d \times r + r \times k = r(d+k)$。当 $r$ 很小时（例如 4, 8, 16），参数量可以减少成百上千倍。

![[Pasted image 20250901100448.png]]

### 2. LoRA 的实现方法

在模型微调时，LoRA 将这种低秩分解思想应用到了模型的关键模块（主要是 Transformer 中的自注意力权重矩阵 $W_q, W_k, W_v, W_o$）上。

对于一个预训练权重为 $W_0$ 的线性层，其前向传播过程为 $h = W_0 x$。应用 LoRA 后，其前向传播变为：

$$
h = W_0 x + \Delta W x = W_0 x + B A x
$$

**关键点：**

-   **冻结预训练权重**：在整个微调过程中，$W_0$ 始终保持不变，不参与任何梯度更新。
-   **只训练 A 和 B**：只有矩阵 $A$ 和 $B$ 是可训练的参数。
-   **旁路结构**：$BAx$ 相当于在原始网络路径旁边增加了一个“旁路”或“残差连接”，用于学习特定任务的适应性调整。

### 3. LoRA 的独特优势

-   **极高的参数效率**：可训练参数量大幅减少，显著降低了显存占用和存储成本。
-   **无额外的推理延迟**：这是 LoRA 相较于其他一些高效微调方法（如 Adapter）的巨大优势。在模型部署推理时，我们可以预先计算好 $W' = W_0 + BA$。推理时，前向传播变为 $h = W'x$，与原始模型相比，没有任何额外的计算步骤。
-   **任务切换灵活**：由于每个任务只对应一对小矩阵 (A, B)，在部署时可以轻松地通过切换不同的 (A, B) 组合，让同一个基础模型服务于不同任务。

### 4. 初始化策略

如何初始化 $A$ 和 $B$ 至关重要。LoRA 论文中采用了非常巧妙的策略：

-   **矩阵 A**：采用**随机高斯分布**进行初始化。这为模型提供了开始学习的随机梯度方向，打破了对称性。
-   **矩阵 B**：采用**全零**进行初始化。

**设计原理：**
当 $B=0$ 时，在训练开始的第一步（$t=0$），$\Delta W = BA = 0$。这意味着微调是从原始的预训练模型 $W_0$ 精确开始的，没有任何随机扰动。这保证了训练的稳定性和一个良好的起点。随着训练的进行，$B$ 会逐渐从零开始学习到有意义的值。

---

## 第二部分：LoRA+

LoRA 虽然高效，但 LoRA+ 的作者们发现了一个可以被优化的细节：矩阵 $A$ 和 $B$ 在 LoRA 中共享同一个学习率，这可能不是最优的。

### 1. 核心思想：非对称的更新需求

LoRA+ 的核心洞察是：**矩阵 $A$ 和 $B$ 的角色和尺度是不同的，因此它们应该使用不同的学习率进行更新。**

-   **矩阵 A ($r \times k$)**：它的作用是将高维的输入特征投影到低维的秩空间。
-   **矩阵 B ($d \times r$)**：它的作用是将低维的秩空间特征投影回高维的输出空间。

直观上，矩阵 $B$ 的更新对最终输出的影响更直接、更剧烈。在具有很大宽度（embedding dimension）的模型中，如果两者使用相同的学习率，会导致特征学习效率低下。

### 2. LoRA+ 的实现方法

LoRA+ 的解决方法简单有效：**为矩阵 B 设置一个远大于矩阵 A 的学习率。**

设矩阵 $A$ 的学习率为 $\eta_A$，矩阵 $B$ 的学习率为 $\eta_B$。LoRA+ 提出：

$$
\eta_B = \lambda \cdot \eta_A \quad \text{其中} \quad \lambda \gg 1
$$

在实践中，$\lambda$ 通常被设置为一个固定的、较大的值（例如 $2^4=16$ 或 $2^5=32$）。研究者只需搜索一个主学习率 $\eta_A$ 即可。

### 3. 梯度与优化的理论分析

考虑模型输出的变化 $\Delta h$，它由两部分贡献：

$$
\Delta h \approx (\Delta B)Ax + B(\Delta A)x
$$

其中，$\Delta B = -\eta_B G_B$ 和 $\Delta A = -\eta_A G_A$。LoRA+ 的理论分析表明，为了让这两部分贡献在数值上保持平衡，学习率需要进行非对称的设置：

-   当模型维度很大时，如果 $\eta_A = \eta_B$，由 $\Delta A$ 引起的更新项在数值上会变得微不足道，导致矩阵 $A$ 无法被有效训练。
-   为了让 $A$ 和 $B$ 都得到充分训练，需要让 $\eta_B$ 的尺度远大于 $\eta_A$。论文推导出最优尺度关系为 $\eta_B = \Theta(1)$ 而 $\eta_A = \Theta(n^{-1})$。

这种修改可以带来约 1%-2% 的性能提升，以及高达 2 倍的训练加速。

## 总结与对比

| 特性 | 全量微调 (Full Fine-Tuning) | LoRA | LoRA+ |
| :--- | :--- | :--- | :--- |
| **核心思想** | 更新所有参数以适应新任务 | 权重更新是低秩的，可用 $BA$ 近似 | $A$ 和 $B$ 作用不同，需要不同更新速率 |
| **可训练参数** | 全部模型参数，数量巨大 | 只有小的矩阵 $A, B$，数量极少 | 和 LoRA 相同 |
| **推理延迟** | 无 | **无** (可合并权重) | **无** (可合并权重) |
| **主要超参** | 学习率 | 学习率 $\eta$, 秩 $r$ | 主学习率 $\eta_A$, 秩 $r$, 比例 $\lambda$ |
| **解决的问题** | - | 微调的巨大存储和计算成本 | LoRA 中特征学习效率不高的局限 |

**结论**：
从全量微调到 LoRA，再到 LoRA+，体现了对大模型动力学理解的深化。LoRA 通过低秩假设实现了参数的高效化，而 LoRA+ 则通过对梯度反向传播路径尺度效应的补偿，进一步优化了训练效率与最终性能。
