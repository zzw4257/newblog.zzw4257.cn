---
title: 算力杂谈 (Ep.2)：微观的物理学 —— 1801 TOPS 与 56 TFLOPS 的罗生门
description: 你手中的 RTX 5080 标称 1801 AI TOPS，但 FP32 只有 56.3 TFLOPS。这巨大的倍数差是从哪里来的？
pubDate: 2026-01-27T10:00:00
author: zzw4257
series: "Compute-2026-DeepDive"
chapter: 2
image: /images/tensor-core-micro.jpg
badge: Hardcore
---

在第一章，我们谈论了宏观架构。现在，让我们把目光聚焦到你机箱里的那张 **RTX 5080** 上。

如果你仔细看过它的规格表，会发现两个极其割裂的数字：
1.  **AI TOPS**: 1801 (NVIDIA 官方宣传数据)
2.  **FP32 (Float)**: ~56.3 TFLOPS (传统光栅化/通用计算性能)

**1801 vs 56.3**，两者相差 **32 倍**。算力去哪了？或者是，多出来的算力是哪里变出来的？要回答这个问题，我们需要搞清楚 `FLOPs_MACs.ipynb` 笔记中提到的那个基础物理公式。

## 一、 基础物理量：FLOPs vs FLOPS

这不仅仅是大小写的区别，这是**工作量**与**工作速度**的区别。

* **FLOPs (s 小写)**: **F**loating **P**oint **Op**eration**s**。
    * 定义：浮点运算次数（总量）。
    * 意义：衡量一个**模型**有多大。
* **FLOPS (S 大写)**: **F**loating **P**oint **Op**eration**s** per **S**econd。
    * 定义：每秒浮点运算次数（速度）。
    * 意义：衡量一张**显卡**算得有多快。

### 1. MACs：GPU 的心跳
现代 GPU（如你的 RTX 5080）的底层计算单元并不是在做简单的 $1+1$。它们最擅长的是 **FMA (Fused Multiply-Add)** 指令：

$$ A \times X + B $$

这一步操作包含了一次乘法、一次加法。在硬件设计中，这被称为 **1 MAC (Multiply-Accumulate Operation)**。

$$ 1 \text{ MAC} = 2 \text{ FLOPs} $$

这就是为什么你在看论文时，有时候计算量写的是 GMACs，有时候是 GFLOPs，两者通常相差 2 倍。

---

## 二、 拆解 RTX 5080：32 倍的差距从何而来？

为什么 RTX 5080 的 **FP32** 只有 56.3 TFLOPS（甚至低于 RTX 4090 的 ~83 TFLOPS），但 **AI TOPS** 却高达 1801？

这正是 **Blackwell 架构** 的激进之处：它牺牲了一部分通用计算（CUDA Core）的堆料，极度强化了 AI 专用电路（Tensor Core）。

### 1. 传统的 CUDA Core (FP32)
* **数值**：56.3 TFLOPS
* **用途**：这是显卡做物理模拟（Isaac Gym）、传统渲染、或者跑没有适配 Tensor Core 的老代码时的真实速度。
* **现状**：在 5080 上，这部分性能相比 4090 甚至有所回缩。这意味着如果你是用它打老游戏或者跑旧的科学计算，提升可能不如预期。

### 2. Tensor Core 的魔法 (AI TOPS)
* **数值**：1801 AI TOPS
* **来源**：这是 **INT8 精度** 下的 **稀疏 (Sparsity)** 性能。

NVIDIA 的宣传逻辑是这样的（倍率魔法）：

1.  **精度降维 (Precision)**:
    Tensor Core 跑 **INT8**（8位整数）的速度通常是 **FP16** 的 2 倍，是 **FP32** 的数倍。Blackwell 架构引入了 **FP4**，速度更是翻倍。
    > 1801 这个数字，通常是在 INT8 甚至更低精度下的峰值。

2.  **稀疏算力 (Sparsity)**:
    NVIDIA 的 Tensor Core 支持 **2:4 Sparsity** 特性。神经网络中有很多参数接近于 0，硬件可以每 4 个数里只算 2 个非零的。
    > **这直接让算力数字 × 2**。

**还原公式**：
你看到的 "1801"，实际上可能是：
$$ \text{基础算力} \times \text{精度倍率 (FP32 \to INT8)} \times \text{稀疏倍率 (2)} \approx 1801 $$

**结论**：在跑 LLM 推理（尤其是量化后的模型）时，5080 是一头野兽，但在跑需要高精度 FP32 的任务时，它是一张常规的高端卡。

---

## 三、 精度战争：FP16 vs FP8 vs FP4

为什么 2026 年的显卡都在卷 FP8 和 FP4？因为对于 LLM 来说，精度不仅是算力问题，更是**显存容量**问题。

* **FP32 (单精度)**: 传统的王者。32-bit。精度高，但慢，占显存。
* **TF32 (Tensor Float 32)**: Ampere 架构引入。截断了 FP32 的尾巴，换取了 **8倍** 于 FP32 的吞吐。
* **FP8**: H200 和 RTX 4090 的杀手锏。显存占用减半。
* **FP4 (Blackwell)**: **RTX 5080 的核心武器**。
    * 你的 5080 只有 16GB 显存。如果是 FP16，只能塞下 8B 模型。
    * 但如果是 **FP4**，16GB 显存理论上可以塞下 **接近 30B** 的模型。
    * 这就是为什么 5080 敢标榜自己是 AI 卡——它用极低的精度（FP4）换取了原本装不下的模型运行能力，并配合 Tensor Core 跑出了 1801 TOPS 的速度。

---

## 四、 总结：如何计算训练时间？

回到你手中的 **RTX 5080**。我们不要看 1801 (INT8)，也不要看 56.3 (FP32)。
在实际训练（Training）中，我们通常使用 **BF16/FP16** 且不开启稀疏化（Dense）。

根据架构推算，5080 的 **Dense BF16 Tensor TFLOPS** 大约在 **100~150 TFLOPS** 之间（保守估计）。

假设你想微调一个 Llama-3-8B 的模型，数据量是 10B Tokens。根据经验公式：

$$ \text{Training FLOPs} \approx 6 \times \text{Param} \times \text{Token} $$
$$ \text{Total FLOPs} = 6 \times 8 \times 10^9 \times 10 \times 10^9 = 4.8 \times 10^{20} \text{ FLOPs} $$

如果用一张 5080 跑（假设实际利用率 40%，有效算力按 50 TFLOPS 算）：

$$ \text{Time} = \frac{4.8 \times 10^{20}}{50 \times 10^{12}} \approx 9,600 \text{ 秒} \approx 2.6 \text{ 小时} $$

**这就是 FLOPs 的意义**。它告诉你，尽管 5080 的显存不如 3090/4090 大，但只要模型能塞进去（或者用 LoRA），它的计算速度依然是非常可观的。

**下一章预告**：
明白了计算够不够，下一次我们要看**数据能不能喂得进去**。
在第三章**《内存墙的叹息》**中，我们将深度解析为什么你的 M4 Pro 虽然没有 1801 TOPS 这么吓人的数字，却能跑得动 70B 模型，以及 GDDR7 与 Unified Memory 的殊途同归。