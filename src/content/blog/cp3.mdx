---
title: 算力杂谈 (Ep.3)：内存墙的叹息 —— GDDR7、HBM3e 与统一内存的殊途同归
description: 为什么 M4 Pro 算力只有 5080 的零头，却能跑 5080 跑不了的大模型？一切的答案都在带宽公式里。
pubDate: 2026-01-28T10:00:00
author: zzw4257
series: "Compute-2026-DeepDive"
chapter: 3
image: /images/memory-wall-2026.jpg
badge: Critical
---

在第二章，我们计算了**“算得有多快”**。而在这一章，我们要解决更致命的问题：**“数据喂得有多快”**。

对于 LLM（大语言模型）的推理过程（Inference），有一个残酷的物理定律：**每生成一个 Token，必须把模型的所有参数从显存（VRAM）搬运到计算核心（Core）一次。**

如果你理解了这句话，你就会明白为什么 H200 卖那么贵，以及为什么你的 M4 Pro 是一个“奇葩”的存在。

## 一、 核心公式：算力利用率的生死线

我们在 `内存.ipynb` 笔记中提到过带宽公式。在 AI 领域，我们用 **算术强度 (Arithmetic Intensity)** 来描述问题：

$$ \text{Arithmetic Intensity} = \frac{\text{FLOPs (计算量)}}{\text{Bytes (数据搬运量)}} $$

### 1. LLM 的尴尬
* **卷积神经网络 (ResNet)**：卷积核复用率极高，算术强度大，是 **Compute Bound**（算力受限）。这种任务 5080 跑得飞快。
* **Transformer (Decoder-Only)**：当你做推理时（Batch Size=1），矩阵向量乘法（GEMV）占据主导。参数只用一次就被扔掉。算术强度极低，是典型的 **Memory Bound**（带宽受限）。

### 2. 你的 5080 在等红绿灯
假设你的 **RTX 5080** 核心能每秒算 $5.6 \times 10^{13}$ 次（56 TFLOPS），但显存带宽只有 900 GB/s。
如果计算强度不够，核心就像法拉利在早高峰的二环上——**大多数时间在空转等待数据**。

---

## 二、 三条路线：GDDR7 vs HBM3e vs Unified

为了翻越内存墙，业界走了三条截然不同的路。看看你手里的设备分别代表了什么。

### 1. 暴力的高频：GDDR7 (你的 RTX 5080)
* **策略**：既然路不够宽（位宽 256-bit），那我就把车速飙到极限。
* **数据**：GDDR7 的等效频率高达 **28 Gbps**（甚至可超频至 32 Gbps）。
* **优势**：**Prefill（预填充）阶段极快**。当你把一大段 Prompt 喂进去时，这是并行计算，5080 能瞬间处理完。
* **劣势**：**发热与功耗**。高频意味着高电压，这就是为什么 5080 依然是个电老虎。

### 2. 极致的位宽：HBM3e (NVIDIA H200)
* **策略**：我不飙车，但我修了 100 车道的高速公路。
* **数据**：HBM3e 通过 TSV（硅通孔）堆叠技术，将位宽做到了恐怖的 **6,144-bit**。即便频率只有 GDDR7 的几分之一，总带宽却高达 **4.8 TB/s**。
* **代价**：**贵**。封装难度极高，良品率低。

### 3. 极近的距离：Unified Memory (你的 M4 Pro)
* **策略**：把显存直接焊在 GPU 旁边（片上封装），并且不区分显存和内存。
* **数据**：LPDDR5X-8533，位宽 256-bit (M4 Pro)，带宽 **273 GB/s**。
* **魔法**：**Zero-Copy（零拷贝）**。CPU 预处理完数据，不需要通过 PCIe 发送给 GPU，指针一指，GPU 直接读。

---

## 三、 实战算账：你的显卡能跑多少 Token/s？

这是本章的**高光时刻**。我们不看评测，直接用物理公式推算你手中两台设备的极限推理速度。

**公式**：
$$ \text{Token/s} \approx \frac{\text{显存带宽 (GB/s)}}{\text{模型大小 (GB)}} $$
*(注：这是理论上限，实际上还要扣除 KV Cache 读写和 Kernel 开销，通常打个 6-8 折)*

### 场景 A：运行 Llama-3-8B (FP16 精度)
* **模型体积**：$8 \times 2 = 16 \text{ GB}$。
* **显存需求**：16GB 模型 + KV Cache。你的 **RTX 5080 (16GB)** 刚好处于爆显存的边缘（OOM），可能需要稍微量化到 8-bit 才能跑顺。

我们假设跑 **INT8 量化** (模型体积 8GB)：

* **RTX 5080 (896 GB/s)**:
    $$ \text{Speed} = \frac{896}{8} = 112 \text{ Tokens/s} $$
    *体验：飞快，字是一个个崩出来的，肉眼看不过来。*

* **M4 Pro (273 GB/s)**:
    $$ \text{Speed} = \frac{273}{8} \approx 34 \text{ Tokens/s} $$
    *体验：流畅，快于人类阅读速度。*

**结论**：在小模型上，**5080 的高带宽优势碾压 M4 Pro**。

### 场景 B：运行 Llama-3-70B (Q4 量化)
* **模型体积**：约 40 GB。
* **显存需求**：40GB + KV Cache。

* **RTX 5080 (16GB)**:
    * **结果**：`CUDA Out of Memory`。
    * **补救**：你只能用“系统内存卸载”（Offload），走 PCIe 4.0 x16 通道（带宽仅 32 GB/s）。
    * **Offload 速度**：$32 / 40 = 0.8 \text{ Tokens/s}$。
    * *体验：卡顿到无法使用，像是在发呆。*

* **M4 Pro (48GB)**:
    * **结果**：**完美装入**。
    * **速度**：
    $$ \text{Speed} = \frac{273}{40} \approx 6.8 \text{ Tokens/s} $$
    * *体验：虽然不快，但每秒能出 6-7 个字，完全可用！*

**这就是 M4 Pro 存在的意义**。它不是为了和 5080 比百米冲刺，它是为了**把 5080 搬不动的货物（大模型）搬起来**。

---

## 四、 隐形瓶颈：Context 越长，M4 Pro 越累

虽然 M4 Pro 能跑 70B，但一定要注意 **KV Cache（键值缓存）**。

在 `内存.ipynb` 中我们忽略了一个变量：随着对话进行，历史记录（Context）越来越长，KV Cache 占用的显存会线性增长。

* **对于 5080**：它的 GDDR7 延迟极低，处理 KV Cache 的随机读写非常利索。
* **对于 M4 Pro**：LPDDR5X 的延迟略高于 GDDR7。当上下文达到 8k 或 16k 时，你会发现 M4 Pro 的输出速度开始出现“一顿一顿”的现象。

**实测建议**：
在 M4 Pro 上运行 70B 模型时，建议将 Context Window 限制在 8k 以内，否则带宽会被 KV Cache 的吞吐挤占，导致生成速度跌破 3 Tokens/s 的“不可忍受线”。

---

## 五、 总结：带宽决定了你的“阶级”

在 2026 年的 AI 硬件鄙视链中，**带宽**就是地位。

1.  **H200 (4.8 TB/s)**：也是唯一的真神，它让计算核心永远吃得饱。
2.  **RTX 5090 (1.8 TB/s)**：消费级的上限，能勉强喂饱 FP8/FP4 的 Tensor Core。
3.  **RTX 5080 (0.9 TB/s)**：**训练小钢炮**。在显存不爆的情况下，它是性价比最高的训练卡。
4.  **M4 Pro (0.27 TB/s)**：**大模型收容所**。它的带宽很低，但它的“仓库”（显存）很大且互通。它是离线运行大参数模型的唯一经济适用方案。

**下一章预告**：
算力和带宽都到位了，为什么多卡训练时 5080 还是跑不过 A800？为什么 NVMe 硬盘的读写速度会影响训练的 epoch 时间？
第四章**《木桶的短板》**，我们将离开 GPU 核心，去检查主板上的 PCIe 通道、CPU 负载以及那个至关重要的 NVLink 桥接器。